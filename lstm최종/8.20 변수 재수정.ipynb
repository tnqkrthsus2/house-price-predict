{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시공사 랭킹 변수가 포함됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from haversine import haversine\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "import random\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  n_vars = 1 if type(data) is list else data.shape[1]\n",
    "  df = pd.DataFrame(data)\n",
    "  cols, names = list(), list()\n",
    "  # input sequence (t-n, ... t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "      cols.append(df.shift(i))\n",
    "      names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "          names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "          names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "  # put it all together\n",
    "  agg = pd.concat(cols, axis=1)\n",
    "  agg.columns = names\n",
    "  # drop rows with NaN values\n",
    "  if dropnan:\n",
    "      agg.dropna(inplace=True)\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기준 아파트로부터 'near_range' km 내에 존재하는 아파트 코드 반환\n",
    "def finding_near_apt(gizun,near_range):\n",
    "    start = (float(gizun['GPS_Y']),float(gizun['GPS_X']))\n",
    "    near_apts = []\n",
    "    for j in df_range.index:\n",
    "        goal = (float(df_range['GPS_Y'][j]),float(df_range['GPS_X'][j]))\n",
    "        if haversine(start, goal) < near_range:\n",
    "            near_apts.append(df_range['apart_code'][j])\n",
    "    near_df = pd.DataFrame()\n",
    "    near_df['apart_code'] = near_apts\n",
    "    return near_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사용하지 않을 과거 데이터 제거(윈도우 사이즈가 3일경우)\n",
    "def delete_unuse_var(reframed):\n",
    "    var_count = int(len(reframed.columns)/4)\n",
    "    reframed.drop(reframed.columns[var_count*2:(var_count*3) - 1], axis=1, inplace=True)\n",
    "    reframed.drop(reframed.columns[var_count:(var_count*2) - 1], axis=1, inplace=True)\n",
    "    reframed.drop(reframed.columns[0:var_count - 1], axis=1, inplace=True)\n",
    "    return reframed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#만약 윈도우 사이즈가 15라면?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터프레임에서 일정한 컬럼만 추출하고 값 형태로 반환\n",
    "def get_float_values(df):\n",
    "    df = df[['YYYYMM','sale_type', 'area_m3', 'floor', 'const_year', \n",
    "    'parking', 'building_year', 'market_dist','busstop_dist', 'subway_dist', \n",
    "    'kinder_dist', 'hospital_dist','near_element', \n",
    "    'near_middle', 'near_high', 'generation','const_rank','prices']]\n",
    "    values = df.values\n",
    "    values = values.astype('float32')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#코드들로부터 값을 추출하고 합치기\n",
    "def get_from_codes(codes):\n",
    "    for j in codes:\n",
    "        df1 = globals()['df_'+j]\n",
    "        values = get_float_values(df1)\n",
    "        reframed = series_to_supervised(values, 3, 1)\n",
    "        reframed = delete_unuse_var(reframed)\n",
    "        global df_all\n",
    "        df_all = pd.concat([reframed,df_all])\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final6.csv',encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결측값 제거\n",
    "df = df[df['sale_type'].notna()]\n",
    "df = df[df['parking'].notna()]\n",
    "df = df[df['HAI'].notna()]\n",
    "df = df[df['M2'].notna()]\n",
    "#정성적변수 정량적 변수로 변환\n",
    "df.loc[df['sale_type'] == '분양', 'sale_type'] = 0\n",
    "df.loc[df['sale_type'] == '임대+분양', 'sale_type'] = 1\n",
    "df.loc[df['sale_type'] == '임대', 'sale_type'] = 2\n",
    "df.loc[df['sale_type'] == '영구임대', 'sale_type'] = 3\n",
    "df['area_m3']=df['area_m3'].round(-1)\n",
    "df = df.astype({'prices':'float64'})\n",
    "df['prices']=df.groupby(['YYYYMM','apart_code','area_m3'])['prices'].transform('mean')\n",
    "df['apart_code_area'] = df['apart_code'].map(str) +'_' +df['area_m3'].map(int).map(str)\n",
    "df_a = df\n",
    "exiting_code = df['apart_code'].unique()\n",
    "df_vary = df.drop_duplicates(['apart_code'])\n",
    "df_range = df_vary[['apart_code','GPS_X','GPS_Y']]\n",
    "many_df = pd.read_csv('서울시아파트(기준).csv',encoding='cp949')\n",
    "many_df = many_df[many_df['k-아파트코드'].isin(exiting_code)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 돌아가는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'A10024725_60'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25744\\3593125813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mdeleted_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'df_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgizun_code_area\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mcodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdeleted_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_from_codes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'var1(t)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m202000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgizun_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25744\\3073179053.py\u001b[0m in \u001b[0;36mget_from_codes\u001b[1;34m(codes)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'df_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_float_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mreframed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries_to_supervised\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mreframed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelete_unuse_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreframed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25744\\2993726979.py\u001b[0m in \u001b[0;36mget_float_values\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      6\u001b[0m     'near_middle', 'near_high', 'generation','const_rank','prices']]\n\u001b[0;32m      7\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'A10024725_60'"
     ]
    }
   ],
   "source": [
    "#6분정도 걸리는듯\n",
    "r2s = []\n",
    "inv_ys = []\n",
    "inv_yhats = []\n",
    "#exiting_code = df['apart_code'].unique()\n",
    "test_codes = random.sample(list(exiting_code),10)\n",
    "for i in test_codes:\n",
    "    gizun_code = i\n",
    "    gizun = df_range[df_range['apart_code']==gizun_code]\n",
    "    #변수 변경시 거리 조정\n",
    "    near_df = finding_near_apt(gizun,2)\n",
    "    near_df = pd.merge(near_df,many_df,left_on='apart_code',right_on='k-아파트코드',how='left')\n",
    "    near_df = near_df[['apart_code','k-전체세대수']]\n",
    "    gizun_count = near_df[near_df['apart_code']==gizun_code].iloc[0][1]\n",
    "    indexNames = near_df[near_df['k-전체세대수'] < gizun_count].index\n",
    "    near_df.drop(indexNames , inplace=True)\n",
    "    df = df_a\n",
    "    df = pd.merge(near_df,df,on='apart_code')\n",
    "    df.pop('k-전체세대수')\n",
    "    df = df.drop_duplicates(subset=['apart_code_area','YYYYMM'])\n",
    "    df = df.sort_values(by=[\"apart_code_area\", \"YYYYMM\"], ascending=[True, True])\n",
    "    df = df[['apart_code_area', 'YYYYMM','sale_type', 'area_m3', 'floor', 'const_year', \n",
    "    'parking', 'building_year', 'market_dist','busstop_dist', 'subway_dist', \n",
    "    'kinder_dist', 'hospital_dist','YYYY', 'MM','near_element', \n",
    "    'near_middle', 'near_high', 'generation','const_rank','prices']]\n",
    "    codes = df['apart_code_area'].unique()\n",
    "    groups = df.groupby(df.apart_code_area)\n",
    "    for j in codes:\n",
    "        globals()['df_'+j] = groups.get_group(j)\n",
    "    gizun_code_areas = df[df['apart_code_area'].str.contains(gizun_code)]['apart_code_area'].unique()\n",
    "    for k in gizun_code_areas:\n",
    "        gizun_code_area = k\n",
    "        gizun_df = globals()['df_'+gizun_code_area]\n",
    "        df_all = pd.DataFrame()\n",
    "        deleted_code = 'df_'+gizun_code_area\n",
    "        codes = np.delete(codes, np.where(codes == deleted_code))\n",
    "        df_all = get_from_codes(codes)\n",
    "        train_set = df_all[df_all['var1(t)']<202000]\n",
    "        df_test = gizun_df\n",
    "        values = get_float_values(df_test)\n",
    "        reframed = series_to_supervised(values, 3, 1)\n",
    "        reframed = delete_unuse_var(reframed)\n",
    "        test_set = reframed[reframed['var1(t)']>=202000]\n",
    "        df_set = pd.concat([train_set,test_set])\n",
    "        train_days =  len(train_set)\n",
    "        values= df_set.values\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        values = scaled\n",
    "        train_days =  len(train_set)\n",
    "        #테스트 데이터 얼마나 사용할 건지\n",
    "        train = values[:train_days]\n",
    "        test = values[train_days:]\n",
    "        # split into input and outputs\n",
    "        train_X, train_y = train[:, :-1], train[:, -1]\n",
    "        test_X, test_y = test[:, :-1], test[:, -1]\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "        test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        history = model.fit(train_X, train_y, epochs=50, batch_size=72, \n",
    "                validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # make a prediction\n",
    "        try:\n",
    "            yhat = model.predict(test_X)\n",
    "            test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "            inv_yhat = concatenate((test_X[:, 0:],yhat), axis=1)\n",
    "            inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "            inv_yhat = inv_yhat[:,-1]\n",
    "            # invert scaling for actual\n",
    "            test_y = test_y.reshape((len(test_y), 1))\n",
    "            inv_y = concatenate((test_X[:, 0:],test_y), axis=1)\n",
    "            inv_y = scaler.inverse_transform(inv_y)\n",
    "            inv_y = inv_y[:,-1]\n",
    "            # calculate RMSE\n",
    "            RMSE = mean_squared_error(inv_y, inv_yhat)**0.5\n",
    "            r2 = r2_score(inv_y, inv_yhat)\n",
    "            r2s.append(r2)\n",
    "            inv_ys.append(inv_y)\n",
    "            inv_yhats.append(inv_yhat)\n",
    "            #만약 해당 값을 데이터 프레임에 넣고 싶다면\n",
    "        except:\n",
    "            #만약 해당 아파트의 2020년 이후 거래내역이 존재하지 않을 경우 오류 발생\n",
    "            print('오류 발생') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(list(itertools.chain(*inv_ys)),list(itertools.chain(*inv_yhats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9016157507313883"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3km일떄\n",
    "#r2_score(list(itertools.chain(*inv_ys)),list(itertools.chain(*inv_yhats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812466131797816"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 모델의 예측값과 실제값을 모아서 2차원리스트를 1차원 리스트로 변환해서 r2값에 넣어버리기\n",
    "# 2km일때 r2_score(list(itertools.chain(*inv_ys)),list(itertools.chain(*inv_yhats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8684070802505793"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2km일때\n",
    "r2_score(list(itertools.chain(*inv_ys)),list(itertools.chain(*inv_yhats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7590976890421938"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1km일떄\n",
    "#r2_score(list(itertools.chain(*inv_ys)),list(itertools.chain(*inv_yhats)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df8ae120c339fa923d6b2ef99afff93aef6b43c9ded0da5150fd70ad6113f611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
